{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr2aJz1PF8mZdBv57BGDpj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z8-wcyNsyiJb"
      },
      "outputs": [],
      "source": [
        "# Problem Statement\n",
        "# Implement Gradient Descent Algorithm to find the local minima of a function.\n",
        "# For example, find the local minima of the function y=(x+3)² starting from the point x=2.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function y = (x + 3)²\n",
        "\n",
        "def fun_minimise(x):\n",
        "  return (x+3)** 2\n",
        "\n",
        "# Define the derivative of the function\n",
        "def gradient(x):\n",
        "  return 2 *(x+3)\n",
        "\n",
        "# Initialise starting point\n",
        "x = 2\n",
        "\n",
        "# Set Parameter\n",
        "learning_rate = 0.1\n",
        "num_iteration = 100\n",
        "\n",
        "#Gradient Descent Algorithm\n",
        "for i in range(num_iteration):\n",
        "  #Calculate gradient at current point\n",
        "  grad = gradient(x)\n",
        "\n",
        "  #Update value x using gradient and learning rate\n",
        "  x = x - learning_rate * grad\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(\"Iteration:\",i,\" x=\",x,\"y=\",fun_minimise(x))\n",
        "\n",
        "#Final value of x will be local minima\n",
        "print(\"Local Minima: x= \", x,\"y=\",fun_minimise(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-rlOb2Zy2C-",
        "outputId": "c0ece6bb-8f79-45fd-d39f-f4c9b1a97d48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0  x= 1.0 y= 16.0\n",
            "Iteration: 10  x= -2.5705032704 y= 0.1844674407370954\n",
            "Iteration: 20  x= -2.953883139815726 y= 0.002126764793255884\n",
            "Iteration: 30  x= -2.995048239842858 y= 2.451992865385725e-05\n",
            "Iteration: 40  x= -2.999468308801686 y= 2.826955303647891e-07\n",
            "Iteration: 50  x= -2.9999429100922916 y= 3.259257562149415e-09\n",
            "Iteration: 60  x= -2.9999938700178364 y= 3.757668132666189e-11\n",
            "Iteration: 70  x= -2.999999341798177 y= 4.3322963956744853e-13\n",
            "Iteration: 80  x= -2.9999999293261177 y= 4.994797639633387e-15\n",
            "Iteration: 90  x= -2.9999999924114498 y= 5.758609463330129e-17\n",
            "Local Minima: x=  -2.999999998981482 y= 1.0373792396055266e-18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2NECROL0Vfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ज्ञान की बातें**"
      ],
      "metadata": {
        "id": "aibrijry0xXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate:** This hyperparameter controls the step size of each update in the gradient descent process. It determines how quickly or slowly the algorithm converges.You can adjust the learning rate to influence the convergence behavior, but selecting an appropriate learning rate is critical, as a value that is too small may lead to slow convergence, and a value that is too large may result in divergence.\n",
        "\n",
        "**num_iteartion:** This hyperparameter sets the number of iterations the gradient descent algorithm will perform. It determines how many times the algorithm updates the value of x while searching for the local minimum. You can adjust the number of iterations based on your problem and the desired level of convergence.\n",
        "\n"
      ],
      "metadata": {
        "id": "bvQUhs7G04h3"
      }
    }
  ]
}